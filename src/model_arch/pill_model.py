import torch
import torch.nn as nn
import torch.nn.functional as F
import timm

# --- 1. Class ‡πÄ‡∏™‡∏£‡∏¥‡∏° (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏ä‡πâ ‡πÅ‡∏ï‡πà‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ) ---
class GeM(nn.Module):
    def __init__(self, p=3, eps=1e-6):
        super(GeM, self).__init__()
        self.p = nn.Parameter(torch.ones(1) * p)
        self.eps = eps
    def forward(self, x):
        return F.avg_pool2d(x.clamp(min=self.eps).pow(self.p), (x.size(-2), x.size(-1))).pow(1./self.p)

class CBAM(nn.Module):
    # (‡πÄ‡∏Å‡πá‡∏ö Class CBAM ‡πÑ‡∏ß‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)
    def __init__(self, planes):
        super(CBAM, self).__init__()
        # ... (‡πÉ‡∏™‡πà Code CBAM ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡∏Å‡πá‡πÑ‡∏î‡πâ) ...
        pass 

# --- 2. The FINAL PillModel (‡∏ï‡∏£‡∏á‡∏õ‡∏Å 100%) ---
class PillModel(nn.Module):
    def __init__(self, num_classes=1000, model_name='convnext_small', embed_dim=512, use_cbam=False):
        super(PillModel, self).__init__()
        
        # 1. Load Backbone
        self.backbone = timm.create_model(model_name, pretrained=True)
        
        # Check num_features
        if hasattr(self.backbone, 'num_features'):
            n_features = self.backbone.num_features
        else:
            n_features = self.backbone.fc.in_features 
            
        # Remove original head
        self.backbone.reset_classifier(0)
        
        # 2. CBAM (‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏° Default)
        self.use_cbam = use_cbam
        if self.use_cbam:
            self.attention = CBAM(n_features) # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Class CBAM ‡∏ñ‡πâ‡∏≤‡∏£‡∏±‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
        
        # 3. üî• ‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Standard Pooling (AvgPool)
        # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå Weight ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ p ‡∏Ç‡∏≠‡∏á GeM
        self.pooling = nn.AdaptiveAvgPool2d((1, 1)) 
        
        # 4. Projection Layers (‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡∏£‡πà‡∏≤‡∏á)
        self.bn = nn.BatchNorm1d(n_features)
        self.fc = nn.Linear(n_features, embed_dim)
        self.bn_emb = nn.BatchNorm1d(embed_dim)
        
        # Head (‡∏°‡∏µ‡πÑ‡∏ß‡πâ‡∏Å‡∏±‡∏ô Error ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ)
        self.head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        features = self.backbone.forward_features(x)
        
        if self.use_cbam:
            features = self.attention(features)
            
        features = self.pooling(features).flatten(1)
        
        features = self.bn(features)
        features = self.fc(features)
        features = self.bn_emb(features)
        
        return features